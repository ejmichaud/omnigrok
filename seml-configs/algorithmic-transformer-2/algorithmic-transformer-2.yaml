
seml:
  executable: scripts/train-algorithmic-transformer.py
  name: algorithmic-transformer-2
  output_dir: seml-configs/algorithmic-transformer-2/log
  project_root_dir: ../../
  conda_environment: torch-cuda

slurm:
  experiments_per_job: 1
  max_simultaneous_jobs: 400
  sbatch_options:
    gres: gpu:1       # num GPUs
    mem: 6G           # memory
    cpus-per-task: 2  # num cores
    time: 0-07:00     # max time, D-HH:MM

fixed:
  operators: ['/']
  p: 97
  optimization_steps: 1000000
  stop_early: True
  batch_size: 512                 # -1 -> entire dataset, 0 < batch_size < 1 -> fraction of dataset, batch_size > 1 -> batch_size
  n_layers: 2
  n_heads: 4
  d_model: 128
  use_positional_encoding: True
  dropout: 0.0
  non_linearity: 'relu'          # 'relu' or 'gelu'
  training_data_fraction: 0.6

  halve_abelian: False
  only_input_tokens: False

  optimizer: "AdamW"
  embedding_lr: 1e-3
  decoder_lr: 1e-3
  embedding_weight_decay: 0.0
  embedding_initialization_scale: 1.0
  eps: 1e-8

  log_freq: 500
  verbose: False
  embeddings_save_freq: 0
  

grid:
  decoder_weight_decay:
    type: choice
    options:
      - 0.0
      - 1e-4
      - 1e-3
      - 1e-2
      - 1e-1
      - 1.0
  decoder_initialization_scale:  
    type: loguniform
    min: 1e-2
    max: 30
    num: 10
  seed:
    type: choice
    options:
      - 0
      - 1

